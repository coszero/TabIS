# settings for evaluation.

PIS:
  # multi-choice
  - spec_name: mc_pcl_wiki-md    
    dataset: mc-pcl_wiki  
    serialization: md
    shot: one           
    format: prompt-response  
    verbalizer: table-qa-mc    
    metric: [exact-match]    
    url:         
    other_info: 
      - description:
  
  - spec_name: mc_rcl_wiki-md    
    dataset: mc-rcl_wiki  
    serialization: md
    shot: one           
    format: prompt-response  
    verbalizer: table-qa-mc    
    metric: [exact-match]    
    url:         
    other_info: 
      - description:
  
  - spec_name: mc_prl_wiki-md    
    dataset: mc-prl_wiki  
    serialization: md
    shot: one           
    format: prompt-response  
    verbalizer: table-qa-mc    
    metric: [exact-match]    
    url:         
    other_info: 
      - description:
  
  - spec_name: mc_rrl_wiki-md    
    dataset: mc-rrl_wiki  
    serialization: md
    shot: one           
    format: prompt-response  
    verbalizer: table-qa-mc    
    metric: [exact-match]    
    url:         
    other_info: 
      - description:

  - spec_name: mc_pll_wiki-md    
    dataset: mc-pll_wiki  
    serialization: md
    shot: one           
    format: prompt-response  
    verbalizer: table-qa-mc    
    metric: [exact-match]    
    url:         
    other_info: 
      - description:
  
  - spec_name: mc_rll_wiki-md    
    dataset: mc-rll_wiki
    serialization: md
    shot: one           
    format: prompt-response  
    verbalizer: table-qa-mc    
    metric: [exact-match]    
    url:         
    other_info: 
      - description:

SIS:
  - spec_name: totto
    dataset: mc-totto
    serialization: md         # currently available: ['md']
    suffix: hl1
    shot: one                 # fixed
    format: prompt-response   # fixed
    verbalizer: table-qa-mc   # fixed
    metric: [exact-match]     # evaluation metrics see metric.py for all available metrics
    url:                      # None
    other_info: 
      - description: 'This is the test dataset generated from Totto.'
  
  - spec_name: hitab
    dataset: mc-hitab
    serialization: md         # currently available: ['md']
    suffix: hl1
    shot: one                 # fixed
    format: prompt-response   # fixed
    verbalizer: table-qa-mc   # fixed
    metric: [exact-match]     # evaluation metrics see metric.py for all available metrics
    url:                      # None
    other_info: 
      - description: 'This is the test dataset generated from hitab.'
